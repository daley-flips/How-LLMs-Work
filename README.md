# How-LLMs-Work

## 1. Data Preprocessing and Tokenizing
Resources: 
- [Huggingface](https://huggingface.co/learn/llm-course/en/chapter2/4)
- [Coursera](https://www.coursera.org/learn/classification-vector-spaces-in-nlp)

## 2. Transformers
What does the architecture look like?
Resources:
- [DeepLearning.AI](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
- [Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need (classic)](https://arxiv.org/abs/1706.03762)
- [HuggingFace](https://huggingface.co/learn/llm-course/chapter1/3?fw=pt)
- [Andrej](https://youtu.be/zjkBMFhNj_g?si=qpSRvibx9nmFojF7)
- [Andrej again](https://youtu.be/kCc8FmEb1nY?si=ziXytqmCcvIeQRVs)
- [OpenAI](https://arxiv.org/abs/2001.08361)


## 3. Training Transformers
Parallelization and running hella GPUs

## 4. Alignment
GPT-3 --> ChatGPT
- SFT
- RLHF or GRPO reward functions

## 5. Applications
- Prompt Engineering
- Zero-shot, few-shot, CoT
- RAG

## 6. Serving an LLM in prod
- model distallation if needed
- token streaming
- API latency